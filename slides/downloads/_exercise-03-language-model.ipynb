{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Language models and large language models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution: Kolhatkar, Varada (2024) [DSCI 575](https://ubc-mds.github.io/DSCI_575_adv-mach-learn/README.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a name=\"im\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from urllib.request import urlopen\n",
    "from hashlib import sha1\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Kaggle Kernels\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to run this notebook on the cloud using [Kaggle](https://www.kaggle.com). To get started, follow these steps:\n",
    "\n",
    "1. Go to https://www.kaggle.com/kernels\n",
    "\n",
    "2. Make an account if you don't have one, and verify your phone number (to get access to GPUs)\n",
    "3. Select `+ New Notebook`\n",
    "4. Go to `File -> Import Notebook`\n",
    "5. Upload this notebook\n",
    "6. On the right-hand side of your Kaggle notebook, make sure:\n",
    "  \n",
    "  - `Internet` is enabled.\n",
    "\n",
    "Once you've done all your work on Kaggle, you can download the notebook from Kaggle. That way any work you did on Kaggle won't be lost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Text Generation using Markov Models\n",
    "\n",
    "### Character-based Markov model of language\n",
    "<hr>\n",
    "\n",
    "In this exercise, you will write a class `MarkovModel` to `fit` an n-gram model of language and generated text.\n",
    "\n",
    "The starter code below uses the hyperparameter `n`, which represents the state of the Markov model as the last `n` characters of a given string. In Exercise 1, we explored `n=1` (bigram model), where each state of the Markov model was a single character, and the generation of each character was dependent only on the previous character. We aim to incorporate more context to produce more intelligible text. For instance, with `n=3`, the probability distribution for the next character will be based on the preceding three characters. \n",
    "\n",
    "> Note that `n` in the term n-gram does not exactly correspond to the variable `n` in our implementation below. Instead `n` refers to the number of previous time steps to use as a context. For 2-gram (bigram) the value of the variable `n` is 1 which means considering one previous time step as context. For 4-gram the value of `n` is 3 which means considering three previous time steps as context.    \n",
    "\n",
    "To train our model, we record every occurrence of each n-gram and the subsequent character. Then, similar to the approach in naive Bayes, we normalize these counts to probabilities for each n-gram. The `fit` function implements these steps. \n",
    "\n",
    "To generate a new sequence, we start with some initial seed at least of length `n`. You can explicitly pass this seed when you call the `generate` method. By default, we will just use the first `n` characters in the training text as the seed, which are saved at the end of the `fit` function. Then, for the current n-gram we will look up the probability distribution over next characters and sample a character according to this distribution.\n",
    "\n",
    "Attribution: assignment adapted with permission from Princeton COS 126, [_Markov Model of Natural Language_]( http://www.cs.princeton.edu/courses/archive/fall15/cos126/assignments/markov.html). Original assignment was developed by Bob Sedgewick and Kevin Wayne. If you are interested in more background info, you can take a look at the original version. The original paper by Shannon, [A Mathematical Theory of Communication](http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf), essentially created the field of information theory and is one of the best scientific papers ever written (in terms of both impact and readability).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the recipe [dataset](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions)\n",
    "\n",
    "1. Click `+ Add data` at the top right of the notebook.\n",
    "\n",
    "2. Search for 'food-com-recipes-and-user-interactions'. Several datasets will appear. Look for and 'Add' the dataset with the size of 280MB.\n",
    "\n",
    "3. Run the follow cells for preparation of the data and model training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set up model\n",
    "class MarkovModel:\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        Initialize the Markov model object.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n : int\n",
    "            the size of the ngram\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.probabilities_ = None\n",
    "        self.frequencies_ = None\n",
    "        self.starting_chars = None\n",
    "\n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        Fit a Markov model and create a transition matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            a corpus of text\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the first n characters of the training text, as we will use these\n",
    "        # to seed our `generate` function\n",
    "        self.starting_chars = text[: self.n]\n",
    "\n",
    "        # Make text circular so markov chain doesn't get stuck\n",
    "        circ_text = text + text[: self.n]\n",
    "\n",
    "        # Step 1: Compute frequencies\n",
    "        # count the number of occurrences of each letter following a given n-gram\n",
    "        frequencies = defaultdict(Counter)\n",
    "        for i in range(len(text)):\n",
    "            frequencies[circ_text[i : i + self.n]][circ_text[i + self.n]] += 1.0\n",
    "\n",
    "        # Step 2: Normalize the frequencies into probabilities\n",
    "        self.probabilities_ = defaultdict(dict)\n",
    "        for ngram, counts in frequencies.items():\n",
    "            self.probabilities_[ngram][\"symbols\"] = list(counts.keys())\n",
    "            probs = np.array(list(counts.values()))\n",
    "            probs /= np.sum(probs)\n",
    "            self.probabilities_[ngram][\"probs\"] = probs\n",
    "\n",
    "        self.frequencies_ = frequencies  # you never know when this might come in handy\n",
    "    \n",
    "    def generate(self, seq_len, seed=\"\"):\n",
    "        \"\"\"\n",
    "        Using self.starting_chars, generate a sequence of length seq_len\n",
    "        using the transition matrix created in the fit method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_len : int\n",
    "            the desired length of the sequence\n",
    "        seed : str\n",
    "            the seed for text generation\n",
    "        Returns:\n",
    "        ----------\n",
    "        str\n",
    "            the generated sequence\n",
    "        \"\"\"\n",
    "        if not seed:\n",
    "            s = self.starting_chars\n",
    "        else:\n",
    "            s = seed\n",
    "\n",
    "        while len(s) < seq_len:\n",
    "            current_ngram = s[-self.n :]\n",
    "            probs = self.probabilities_[current_ngram]\n",
    "            s += np.random.choice(probs[\"symbols\"], p=probs[\"probs\"])\n",
    "        return s\n",
    "\n",
    "# Set up data\n",
    "recipes_file = \"/kaggle/input/food-com-recipes-and-user-interactions/RAW_recipes.csv\"\n",
    "orig_recipes_df = pd.read_csv(recipes_file)\n",
    "orig_recipes_df = orig_recipes_df.dropna()\n",
    "corpus = \"\\n\".join(orig_recipes_df[\"name\"].tolist())\n",
    "print(f\"Corpus length: {len(corpus)}\")\n",
    "print(f\"Corpus sample: {corpus[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novel recipe name generation\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "In this exercise, you will train Markov models using the recipe titles in the `corpus` variable above for different values of `n` within the range 1 to 10. For each value of `n`, show generated text comprising at least `100` characters by running the follow cells. \n",
    "\n",
    "Feel free to select any `n` and `seed` of your choice. Please note that the length of your `seed` should be at least `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "n = 3\n",
    "num_characters = 100\n",
    "seed = 'Cho'\n",
    "\n",
    "char_model = MarkovModel(n=n)\n",
    "char_model.fit(corpus)\n",
    "print(f\"Text generated with n = {n}:\")\n",
    "print(f\"Generate recipe titles: \\n{char_model.generate(num_characters, seed=seed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1 \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Discussion questions**\n",
    "\n",
    "1. How does the value of `n` influence the quality of the generated recipe titles?\n",
    "2. What would happen to the performance if we change the model from character-based to word-based?   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Type your answer below.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Sentiment Analysis with LLMs\n",
    "<hr>\n",
    "\n",
    "In this exercise you're going to apply some LLM in some real-life use cases. We're going to to build a sentiment analysis model for detecting tweet sentiments using a Kaggle [dataset](https://www.kaggle.com/datasets/saurabhshahane/twitter-sentiment-dataset):\n",
    "\n",
    "In order to use the dataset.\n",
    "\n",
    "1. Click `+ Add data` at the top right of the notebook.\n",
    "\n",
    "2. Search for 'twitter-sentiment-dataset'. Several datasets will appear. Look for and 'Add' the dataset with the size of 8MB.\n",
    "\n",
    "3. Run the follow cell for preparation of the data and model training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set up data\n",
    "tweets_file = \"/kaggle/input/twitter-sentiment-dataset/Twitter_Data.csv\"\n",
    "tweets = pd.read_csv(tweets_file)\n",
    "tweet_content = tweets['clean_text'].tolist()\n",
    "\n",
    "# Set up model\n",
    "model_name = \"facebook/bart-large-mnli\"\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model_name)\n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "\n",
    "def sentiment_analyzer(classifier, text, candidate_labels):\n",
    "    results = classifier(text, candidate_labels)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you want to take a look at the text, try this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tweet_content[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1 \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Discussion questions**\n",
    "\n",
    "1. How well does the model distinguish between cats and dogs?\n",
    "2. Do you notice any specific patterns or characteristics in the labels?\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Type your answer below.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your tasks:**\n",
    "\n",
    "In this exercise, you will use a pre-trained LLM to classify positive/negative tweets based on their content! \n",
    "\n",
    "You will use any pre-trained LLM you wish (`facebook/bart-large-mnli` for this case) and start the classifciation by running the follow cells.\n",
    "\n",
    "A high score in the `scores` column indicates the model predicts a higher probability of the tweet belonging to that sentiment label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "tweet_content_subset = tweet_content[:50]\n",
    "\n",
    "results = sentiment_analyzer(classifier, tweet_content_subset, candidate_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Food for Thought**: \n",
    "\n",
    "- How is the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your tasks:**\n",
    "\n",
    "In this exercise, you will use a pre-trained LLM to classify the sentiment of tweets into different categories based on their content. \n",
    "\n",
    "You can define as many sentiment as you like. Experiment and find which works better.\n",
    "\n",
    "You will start the classification by running the follow cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "candidate_labels = [\"sadness\", \"joy\", \"anger\", \"fear\"]\n",
    "tweet_content_subset = tweet_content[:50]\n",
    "\n",
    "results = sentiment_analyzer(classifier, tweet_content_subset, candidate_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Food for Thought**: \n",
    "\n",
    "- How is the performance of the model? Comment on the performance of this model compared to the binary model.\n",
    "- What do you think about the model performance if you change the labels to other emotions (e.g. surprise, anxiety, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Free Time (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your tasks**:\n",
    "\n",
    "You will add any textual dataset you like, use your own labels by changing the items with **...** and build your own classification model!\n",
    "\n",
    "Feel free to share your thoughts with your teammates and workshop team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "candidate_labels = ...\n",
    "text = ...\n",
    "\n",
    "results = sentiment_analyzer(classifier, text, candidate_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 311962,
     "sourceId": 783630,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1340873,
     "sourceId": 2231927,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
