{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# DSI-ML-workshop: Deep Learning and Transfer Learning"]},{"cell_type":"markdown","metadata":{},"source":["Attribution: Kolhatkar, Varada (2024) DSCI572 "]},{"cell_type":"markdown","metadata":{},"source":["## Imports\n","<hr>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from collections import OrderedDict\n","import torch\n","from torch import nn, optim\n","from torchvision import datasets, transforms, utils, models\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","from PIL import Image\n","\n","plt.rcParams.update({'axes.grid': False})"]},{"cell_type":"markdown","metadata":{},"source":["<br><br>"]},{"cell_type":"markdown","metadata":{},"source":["## Getting Started with Kaggle Kernels\n","<hr>"]},{"attachments":{},"cell_type":"markdown","metadata":{"colab_type":"text","id":"91yxVPKkO2qa"},"source":["We are going to run this notebook on the cloud using [Kaggle](https://www.kaggle.com). Kaggle offers 30 hours of free GPU usage per week which should be much more than enough for this lab. To get started, follow these steps:\n","\n","1. Go to https://www.kaggle.com/kernels\n","\n","2. Make an account if you don't have one, and verify your phone number (to get access to GPUs)\n","3. Select `+ New Notebook`\n","4. Go to `File -> Import Notebook`\n","5. Upload this notebook\n","6. On the right-hand side of your Kaggle notebook, make sure:\n","  \n","  - `Internet` is enabled.\n","  \n","  - In the `Accelerator` dropdown, choose `GPU` when you're ready to use it (you can turn it on/off as you need it).\n","7. In Kaggle Notebook, running the follow cell should print out `\"Using device: cuda\"` which means a GPU is available:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device.type}\")"]},{"cell_type":"markdown","metadata":{},"source":["Once you've done all your work on Kaggle, you can download the notebook from Kaggle. That way any work you did on Kaggle won't be lost. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Exercise 1: Transfer Learning\n","<hr>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this exercise you're going to practice transfer learning. We're going to develop a model that can detect the following 6 cat breeds in this Kaggle [dataset](https://www.kaggle.com/solothok/cat-breed):\n","\n","In order to use this dataset \n","\n","1. Click `+ Add data` at the top right of the notebook.\n","\n","2. Search for 'cat-breed-mardhik'. Several datasets will appear. Look for and 'Add' the dataset with the size of 93 MB."]},{"cell_type":"markdown","metadata":{},"source":["### Out-of-the-box Classification"]},{"cell_type":"markdown","metadata":{},"source":["Before the exercise, you will utilize the power of CNN model to classify some images of cats and dogs based on common cats and dogs image [dataset](https://www.kaggle.com/datasets/tongpython/cat-and-dog)! \n","\n","However, training a CNN model from scratch requires a lot of computation resources. So, you will leverage a pre-trained model (`DenseNet` for this case) to identify the animals!\n","\n","In order to use the dataset.\n","\n","1. Click `+ Add data` at the top right of the notebook.\n","\n","2. Search for 'cat-and-dog'. Several datasets will appear. Look for and 'Add' the first dataset with the size of 228 MB.\n","\n","3. Run the follow cell for preparation of the data, default labels and models."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set up data\n","SAMPLE_DATA_DIR = \"/kaggle/input/cat-and-dog/training_set/training_set\"\n","\n","def data_loader(DIR):\n","    IMAGE_SIZE = 200\n","    BATCH_SIZE = 32\n","    \n","    data_transforms = transforms.Compose([\n","        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","    \n","    dataset = datasets.ImageFolder(root=DIR, transform=data_transforms)\n","    loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","    return loader\n","\n","sample_data_loader = data_loader(SAMPLE_DATA_DIR)\n","\n","def plot_samples(data_loader, model=None, classes=None):\n","    sample_batch = next(iter(data_loader))\n","    plt.figure(figsize=(20, 16)); \n","    plt.axis(\"off\"); \n","    plt.title(\"Sample Images\")\n","    plt.imshow(np.transpose(utils.make_grid(sample_batch[0], padding=1, normalize=True),(1, 2, 0)));\n","    actual_labels = sample_batch[1].numpy()\n","    print(f\"Actual Labels: {actual_labels}\")\n","    if model:\n","        model.to(device)\n","        _, y_hat_labels = torch.softmax(model(sample_batch[0].to(device)), dim=1).topk(1, dim=1)\n","        predicted_labels = y_hat_labels.squeeze().cpu().numpy()\n","        accuracy = np.mean(actual_labels == predicted_labels)\n","        if classes:\n","            predicted_labels = [classes[label] for label in predicted_labels]\n","        print(f\"Predicted Labels: {predicted_labels}\")\n","        print(f\"Accuracy: {accuracy}\")\n","        \n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Download ImageNet labels\n","!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Read the ImageNet categories\n","with open(\"imagenet_classes.txt\", \"r\") as f:\n","    pretrained_categories = [s.strip() for s in f.readlines()]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Download model and freeze params\n","cnn_model = models.densenet121(pretrained=True)\n","for param in cnn_model.parameters():\n","    param.requires_grad = False"]},{"cell_type":"markdown","metadata":{},"source":["Then, you will obtain the predictions (`Predicted Labels`) from the model by running the follow cell. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Time to predict\n","plot_samples(sample_data_loader, cnn_model, pretrained_categories)"]},{"cell_type":"markdown","metadata":{},"source":["**Food for Thought**: \n","\n","- How well can model identify cats and dogs?\n","- Anything you notice about the default labels?"]},{"cell_type":"markdown","metadata":{},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","metadata":{},"source":["Now, we're going to develop a model that can detect the following 6 cat breeds in this Kaggle [dataset](https://www.kaggle.com/solothok/cat-breed):\n","\n","0. American Short hair\n","1. Bengal\n","2. Maine Soon\n","3. Ragdoll\n","4. Scottish Fold\n","5. Sphinx"]},{"cell_type":"markdown","metadata":{},"source":["**Your tasks:**\n","\n","In this exercise, you will build a CNN model to classify images of cats based on their breeds! \n","\n","You will leverage a pre-trained model (`DenseNet` for this case) to identify various cat breeds!\n","\n","First, run the follow cells for preparation of the data."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set up data\n","TRAIN_DIR = \"/kaggle/input/cat-breed/cat-breed/TRAIN/\"\n","VALID_DIR = \"/kaggle/input/cat-breed/cat-breed/TEST/\"\n","\n","train_loader = data_loader(TRAIN_DIR)\n","valid_loader = data_loader(VALID_DIR)"]},{"cell_type":"markdown","metadata":{},"source":["Then, you will obtain the predictions (`Predicted Labels`) from the model by running the follow cell."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Time to predict\n","plot_samples(train_loader, cnn_model, pretrained_categories)"]},{"cell_type":"markdown","metadata":{},"source":["**Food for Thought**: \n","\n","- How is the performance of the model for this dataset?\n","- Do you think it does well in predicting the specific labels that we need?"]},{"cell_type":"markdown","metadata":{},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-11-24T01:31:04.573651Z","iopub.status.busy":"2024-11-24T01:31:04.573302Z","iopub.status.idle":"2024-11-24T01:31:04.578075Z","shell.execute_reply":"2024-11-24T01:31:04.577044Z","shell.execute_reply.started":"2024-11-24T01:31:04.573621Z"}},"source":["### Feature Extractor"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":true,"editable":true},"source":["**Your tasks:**\n","\n","In this exercise, you will train a CNN model customized with your own layer(s) on top in order to build a CNN classifier that can identify specific cat breeds!\n","\n","First, run the follow cell for preparation of the model training setup."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set up trainer\n","def trainer(model, train_loader, valid_loader, epochs=10, verbose=True):\n","    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n","    \n","    train_loss, valid_loss, train_accuracy, valid_accuracy = [], [], [], []\n","    for epoch in range(epochs):  # for each epoch\n","        train_batch_loss = 0\n","        train_batch_acc = 0\n","        valid_batch_loss = 0\n","        valid_batch_acc = 0\n","        \n","        # Training\n","        model.train()\n","        for X, y in train_loader:\n","            X, y = X.to(device), y.to(device)\n","            optimizer.zero_grad()\n","            y_hat = model(X)\n","            _, y_hat_labels = torch.softmax(y_hat, dim=1).topk(1, dim=1)\n","            loss = criterion(y_hat, y)\n","            loss.backward()\n","            optimizer.step()\n","            train_batch_loss += loss.item()\n","            train_batch_acc += (y_hat_labels.squeeze() == y).type(torch.float32).mean().item()\n","        train_loss.append(train_batch_loss / len(train_loader))\n","        train_accuracy.append(train_batch_acc / len(train_loader))\n","        \n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            for X, y in valid_loader:\n","                X, y = X.to(device), y.to(device)\n","                y_hat = model(X)\n","                _, y_hat_labels = torch.softmax(y_hat, dim=1).topk(1, dim=1)\n","                loss = criterion(y_hat, y)\n","                valid_batch_loss += loss.item()\n","                valid_batch_acc += (y_hat_labels.squeeze() == y).type(torch.float32).mean().item()\n","        valid_loss.append(valid_batch_loss / len(valid_loader))\n","        valid_accuracy.append(valid_batch_acc / len(valid_loader))\n","        \n","        # Print progress\n","        if verbose:\n","            print(f\"Epoch {epoch + 1}:\",\n","                  f\"Train Accuracy: {train_accuracy[-1]:.2f}\",\n","                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}\")\n","    \n","    results = {\"train_loss\": train_loss,\n","               \"train_accuracy\": train_accuracy,\n","               \"valid_loss\": valid_loss,\n","               \"valid_accuracy\": valid_accuracy}\n","    return results"]},{"cell_type":"markdown","metadata":{},"source":["> If you want to take a look at the images in training set, try this code:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot samples\n","plot_samples(train_loader)"]},{"cell_type":"markdown","metadata":{},"source":["Then, you will use a pre-trained model (`DenseNet` for this case), define the classification layer for our specific classes, and start training your model by running the follow cells."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Download model and freeze params\n","cnn_model = models.densenet121(pretrained=True)\n","for param in cnn_model.parameters():\n","    param.requires_grad = False\n","\n","# Customize final classification layers\n","new_layers = nn.Sequential(\n","    nn.Linear(1024, 50),\n","    nn.ReLU(),\n","    nn.Linear(50, 6)\n",")\n","cnn_model.classifier = new_layers"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{"tags":["otter_ignore"]},"tags":[],"trusted":true},"outputs":[],"source":["# Time to train\n","results = trainer(cnn_model, train_loader, valid_loader)"]},{"cell_type":"markdown","metadata":{},"source":["> If you want to take a look at the images in validation set and compare the actual and the predicted labels, try this code:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot samples\n","plot_samples(valid_loader, cnn_model)"]},{"cell_type":"markdown","metadata":{},"source":["**Food for Thought**: \n","\n","- How is the performance of the model? Compare the performance of out-of-the-box model and the feature extractor model."]},{"cell_type":"markdown","metadata":{},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","metadata":{},"source":["### Your Free Time (Optional)"]},{"cell_type":"markdown","metadata":{},"source":["**Your tasks**:\n","\n","You will add any image dataset you like and train your own model with your own settings!\n","\n","Feel free to share your thoughts with your teammates and workshop team."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["TRAIN_DIR = \"{_TRAIN_FILE_PATH_}\"\n","VALID_DIR = \"{_VALID_FILE_PATH_}\"\n","# Example\n","# TRAIN_DIR = \"/kaggle/input/cat-breed/cat-breed/TRAIN/\"\n","# VALID_DIR = \"/kaggle/input/cat-breed/cat-breed/TEST/\"\n","\n","train_loader = data_loader(TRAIN_DIR)\n","valid_loader = data_loader(VALID_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot samples\n","plot_samples(train_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_label_classes = \"{_num_label_classes_}\"\n","# Example\n","# num_label_classes = 6\n","\n","# Download model and freeze params\n","cnn_model = models.densenet121(pretrained=True)\n","for param in cnn_model.parameters():\n","    param.requires_grad = False\n","\n","# Customize final classification layers\n","new_layers = nn.Sequential(\n","    nn.Linear(1024, 50),\n","    nn.ReLU(),\n","    nn.Linear(50, num_label_classes)\n",")\n","cnn_model.classifier = new_layers"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Time to train\n","results = trainer(cnn_model, train_loader, valid_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot samples\n","plot_samples(valid_loader, cnn_model)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true},"source":["<!-- END QUESTION -->\n","\n","<br><br>"]},{"cell_type":"markdown","metadata":{},"source":["<br><br>"]}],"metadata":{"accelerator":"GPU","colab":{"name":"lab4.ipynb","provenance":[],"version":"0.3.2"},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":23777,"sourceId":30378,"sourceType":"datasetVersion"},{"datasetId":1758005,"sourceId":2870573,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"otter":{"OK_FORMAT":true,"tests":{}},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"vscode":{"interpreter":{"hash":"f821000d0c0da66e5bcde88c37d59c8e0de03b40667fb62009a8148ca49465a0"}}},"nbformat":4,"nbformat_minor":4}
