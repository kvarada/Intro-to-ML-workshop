{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":783630,"sourceType":"datasetVersion","datasetId":311962},{"sourceId":2231927,"sourceType":"datasetVersion","datasetId":1340873}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"anaconda-cloud":{}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DSI-ML-workshop: Language Model and LLM","metadata":{}},{"cell_type":"markdown","source":"Attribution: Kolhatkar, Varada (2024) DSCI575 ","metadata":{}},{"cell_type":"markdown","source":"## Imports <a name=\"im\"></a>","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport sys\nfrom collections import Counter, defaultdict\nfrom urllib.request import urlopen\nfrom hashlib import sha1\n\nimport numpy as np\nimport numpy.random as npr\nimport pandas as pd\n\nfrom transformers import pipeline, AutoTokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Getting Started with Kaggle Kernels\n<hr>","metadata":{}},{"cell_type":"markdown","source":"We are going to run this notebook on the cloud using [Kaggle](https://www.kaggle.com). To get started, follow these steps:\n\n1. Go to https://www.kaggle.com/kernels\n\n2. Make an account if you don't have one, and verify your phone number (to get access to GPUs)\n3. Select `+ New Notebook`\n<img src=\"img/create_notebook.png\" alt=\"drawing\" width=\"500\"/>\n4. Go to `File -> Import Notebook`\n5. Upload this notebook\n6. On the right-hand side of your Kaggle notebook, make sure:\n  \n  - `Internet` is enabled.\n\nOnce you've done all your work on Kaggle, you can download the notebook from Kaggle. That way any work you did on Kaggle won't be lost. ","metadata":{}},{"cell_type":"markdown","source":"## Exercise 1: Text Generation using Markov Models\n\n### 1.1: Character-based Markov model of language\n<hr>\n\nIn this exercise, you wrote a class `MarkovModel` to `fit` an n-gram model of language and generated text.\n\nThe starter code below uses the hyperparameter `n`, which represents the state of the Markov model as the last `n` characters of a given string. In Exercise 1, we explored `n=1` (bigram model), where each state of the Markov model was a single character, and the generation of each character was dependent only on the previous character. We aim to incorporate more context to produce more intelligible text. For instance, with `n=3`, the probability distribution for the next character will be based on the preceding three characters. \n\n> Note that `n` in the term n-gram does not exactly correspond to the variable `n` in our implementation below. Instead `n` refers to the number of previous time steps to use as a context. For 2-gram (bigram) the value of the variable `n` is 1 which means considering one previous time step as context. For 4-gram the value of `n` is 3 which means considering three previous time steps as context.    \n\nTo train our model, we record every occurrence of each n-gram and the subsequent character. Then, similar to the approach in naive Bayes, we normalize these counts to probabilities for each n-gram. The `fit` function implements these steps. \n\nTo generate a new sequence, we start with some initial seed at least of length `n`. You can explicitly pass this seed when you call the `generate` method. By default, we will just use the first `n` characters in the training text as the seed, which are saved at the end of the `fit` function. Then, for the current n-gram we will look up the probability distribution over next characters and sample a character according to this distribution.\n\nAttribution: assignment adapted with permission from Princeton COS 126, [_Markov Model of Natural Language_]( http://www.cs.princeton.edu/courses/archive/fall15/cos126/assignments/markov.html). Original assignment was developed by Bob Sedgewick and Kevin Wayne. If you are interested in more background info, you can take a look at the original version. The original paper by Shannon, [A Mathematical Theory of Communication](http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf), essentially created the field of information theory and is one of the best scientific papers ever written (in terms of both impact and readability).  ","metadata":{}},{"cell_type":"markdown","source":"In order to use the recipe [dataset](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions)\n\n1. Click `+ Add data` at the top right of the notebook.\n\n<img src=\"img/add_data1.png\" alt=\"drawing\" width=\"300\"/>\n\n2. Search for 'food-com-recipes-and-user-interactions'. Several datasets will appear. Look for and 'Add' the dataset with the specific thumbnail below.\n\n<img src=\"img/add_data3.png\" alt=\"drawing\" width=\"300\"/>\n\n3. Run the follow cells for preparation of the data and model training setup.","metadata":{}},{"cell_type":"code","source":"# Set up model\nclass MarkovModel:\n    def __init__(self, n):\n        \"\"\"\n        Initialize the Markov model object.\n\n        Parameters:\n        ----------\n        n : int\n            the size of the ngram\n        \"\"\"\n        self.n = n\n        self.probabilities_ = None\n        self.frequencies_ = None\n        self.starting_chars = None\n\n    def fit(self, text):\n        \"\"\"\n        Fit a Markov model and create a transition matrix.\n\n        Parameters\n        ----------\n        text : str\n            a corpus of text\n        \"\"\"\n\n        # Store the first n characters of the training text, as we will use these\n        # to seed our `generate` function\n        self.starting_chars = text[: self.n]\n\n        # Make text circular so markov chain doesn't get stuck\n        circ_text = text + text[: self.n]\n\n        # Step 1: Compute frequencies\n        # count the number of occurrences of each letter following a given n-gram\n        frequencies = defaultdict(Counter)\n        for i in range(len(text)):\n            frequencies[circ_text[i : i + self.n]][circ_text[i + self.n]] += 1.0\n\n        # Step 2: Normalize the frequencies into probabilities\n        self.probabilities_ = defaultdict(dict)\n        for ngram, counts in frequencies.items():\n            self.probabilities_[ngram][\"symbols\"] = list(counts.keys())\n            probs = np.array(list(counts.values()))\n            probs /= np.sum(probs)\n            self.probabilities_[ngram][\"probs\"] = probs\n\n        self.frequencies_ = frequencies  # you never know when this might come in handy\n    \n    def generate(self, seq_len, seed=\"\"):\n        \"\"\"\n        Using self.starting_chars, generate a sequence of length seq_len\n        using the transition matrix created in the fit method.\n\n        Parameters\n        ----------\n        seq_len : int\n            the desired length of the sequence\n        seed : str\n            the seed for text generation\n        Returns:\n        ----------\n        str\n            the generated sequence\n        \"\"\"\n        if not seed:\n            s = self.starting_chars\n        else:\n            s = seed\n\n        while len(s) < seq_len:\n            current_ngram = s[-self.n :]\n            probs = self.probabilities_[current_ngram]\n            s += np.random.choice(probs[\"symbols\"], p=probs[\"probs\"])\n        return s\n\n# Set up data\nrecipes_file = \"/kaggle/input/food-com-recipes-and-user-interactions/RAW_recipes.csv\"\norig_recipes_df = pd.read_csv(recipes_file)\norig_recipes_df = orig_recipes_df.dropna()\ncorpus = \"\\n\".join(orig_recipes_df[\"name\"].tolist())\nprint(f\"Corpus length: {len(corpus)}\")\nprint(f\"Corpus sample: {corpus[:100]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Novel recipe name generation\n\n**Your tasks:**\n\nIn this exercise, you will train Markov models using the recipe titles in the `corpus` variable above for different values of `n` within the range 1 to 10. For each value of `n`, show generated text comprising at least `100` characters by running the follow cells. \n\nFeel free to select any `n` and `seed` of your choice. Please note that the length of your `seed` should be at least `n`.","metadata":{}},{"cell_type":"code","source":"# Settings\nn = 3\nnum_characters = 100\nseed = 'egg'\n\nchar_model = MarkovModel(n=n)\nchar_model.fit(corpus)\nprint(f\"Text generated with n = {n}:\")\nprint(f\"Generate recipe titles: \\n{char_model.generate(num_characters, seed=seed)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Food for Thought**: \n\n- How does the value of `n` influence the quality of the generated recipe titles?","metadata":{}},{"cell_type":"markdown","source":"_Type your answer here, replacing this text._","metadata":{"execution":{"iopub.status.busy":"2024-11-24T23:15:21.190797Z","iopub.execute_input":"2024-11-24T23:15:21.191236Z","iopub.status.idle":"2024-11-24T23:15:21.198859Z","shell.execute_reply.started":"2024-11-24T23:15:21.191200Z","shell.execute_reply":"2024-11-24T23:15:21.197277Z"}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### 1.2: Markov model of language with words\n<hr>\n\nIn this exercise we'll continue with the Markov model of language, but we'll work with _words_ instead of _characters_. The `MarkovModel` code stays the same. Just remember that now we are dealing with words and not characters. \n\nWhen we say n-gram we're now referring to `n` previous **words**, not `n` characters.\n\nOne concern with words is that, in a sense, we have less examples to work with. For example, with character n-grams and `n=3` we have a lot of examples of seeing the character combination \"cak\" and checking what character follows it, but with words even with `n=1` (the smallest nontrivial value of `n`), we might only have one example of the word \"personification\" in our corpus. You can imagine that the number of \"training examples\" only gets smaller if `n`>1. This is something to keep in mind when deciding what values of `n` might be reasonable.\n\nAnother issue is that the number of states could explode when you are working with words. For example, with character bigram model our states were all unique characters in the text (38 unique characters in our example above). For a word bigram model, it's going to be the number of unique **words** in the corpus, which would be a much bigger number. You can imagine that for a large corpus and bigger values of `n`, the number of states could explode.  \n\nFinally, we need to preprocess the text (i.e., tokenization) before creating a word-based n-gram model. To accomplish this preprocessing, we will use the popular python package [NLTK](http://www.nltk.org/) (Natural Language ToolKit).","metadata":{}},{"cell_type":"markdown","source":"### Word-based Markov models text generation\n**Your tasks:**\n\nIn this exercise, you will implement a word-based Markov model of language for different values of `n` within the range 1 to 5. \n\nFirst, run the follow cell for preparation of the data and model training setup. ","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nimport string\n\n# NLTK's `word_tokenize` will turn our text into a lists of \"tokens\" which we will use as our language units.\ntext_tok = word_tokenize(corpus)\ntext_tok = tuple(text_tok)\npunc = string.punctuation + \"”\" + \"’\"\n\nclass MarkovModelWords(MarkovModel):\n    def generate(self, seq_len, seed=''):\n        n = self.n\n        if not seed:\n            seq = self.starting_chars\n        else:\n            seq = seed\n        print(f'seq: {seq}')\n        while len(seq) < seq_len:\n            probs = self.probabilities_[seq[-n:]]\n            seq += (npr.choice(probs[\"symbols\"], p=probs[\"probs\"]),)\n        output = seq[0]\n        for s in seq[1:]:\n            if s[0] in punc:  # string.punctuation:\n                # if any(x in string.punctuation for x in s):\n                output += s\n            else:\n                output += \" \" + s\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For each value of `n`, show generated text with at least `20` words by running the follow cell. \n\nFeel free to select any `n` and `seed` of your choice. Please note that your `seed` should be at least `n` long and should exist in the recipe corpus.","metadata":{}},{"cell_type":"code","source":"# Settings\nn = 2\nnum_words = 20\nseed = ('apple', 'pie')\n\nword_model = MarkovModelWords(n=n)\nword_model.fit(text_tok)\nprint(f\"Text generated with n = {n}:\")\nprint(f\"Generate recipe titles: \\n{word_model.generate(num_words, seed=seed)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Food for Thought**: \n\n- Is there any differences in the quality of the generated text between character-based and word-based Markov models?\n- How about the time taken to generate the text?","metadata":{}},{"cell_type":"markdown","source":"_Type your answer here, replacing this text._","metadata":{}},{"cell_type":"markdown","source":"## Exercise 2: Sentiment Analysis with LLMs\n<hr>\n\nIn this exercise you're going to apply some non-GPT LLM in some real-life use cases. We're going to to build a sentiment analysis model for detecting tweet sentiments using a Kaggle [dataset](https://www.kaggle.com/datasets/saurabhshahane/twitter-sentiment-dataset):\n\nIn order to use the dataset.\n\n1. Click `+ Add data` at the top right of the notebook.\n\n<img src=\"img/add_data1.png\" alt=\"drawing\" width=\"300\"/>\n\n2. Search for 'twitter-sentiment-dataset'. Several datasets will appear. Look for and 'Add' the dataset with the specific thumbnail below.\n\n<img src=\"img/add_data4.png\" alt=\"drawing\" width=\"300\"/>\n\n3. Run the follow cell for preparation of the data and model training setup.","metadata":{}},{"cell_type":"code","source":"# Set up data\ntweets_file = \"/kaggle/input/twitter-sentiment-dataset/Twitter_Data.csv\"\ntweets = pd.read_csv(tweets_file)\ntweet_content = tweets['clean_text'].tolist()\n\n# Set up model\nmodel_name = \"facebook/bart-large-mnli\"\nclassifier = pipeline(\"zero-shot-classification\", model=model_name)\ncandidate_labels = [\"positive\", \"negative\"]\n\ndef sentiment_analyzer(classifier, text, candidate_labels):\n    results = classifier(text, candidate_labels)\n    return pd.DataFrame(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Binary Sentiment Analyzer","metadata":{}},{"cell_type":"markdown","source":"**Your tasks:**\n\nIn this exercise, you will use a pre-trained LLM to classify positive/negative tweets based on their content! \n\nYou will use any pre-trained LLM you wish (`facebook/bart-large-mnli` for this case) and start the classifciation by running the follow cells.\n\nA high score in the `scores` column indicates the model predicts a higher probability of the tweet belonging to that sentiment label.","metadata":{}},{"cell_type":"code","source":"candidate_labels = [\"positive\", \"negative\"]\ntweet_content_subset = tweet_content[:50]\n\nresults = sentiment_analyzer(classifier, tweet_content_subset, candidate_labels)\nresults","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Food for Thought**: \n\n- How is the performance of the model?","metadata":{}},{"cell_type":"markdown","source":"_Type your answer here, replacing this text._","metadata":{}},{"cell_type":"markdown","source":"### Multiclass Sentiment Analyzer","metadata":{}},{"cell_type":"markdown","source":"**Your tasks:**\n\nIn this exercise, you will use a pre-trained LLM to classify the sentiment of tweets into different categories based on their content. \n\nYou can define as many sentiment as you like. Experiment and find which works better.\n\nYou will start the classification by running the follow cells.","metadata":{}},{"cell_type":"code","source":"candidate_labels = [\"sadness\", \"joy\", \"anger\", \"fear\"]\ntweet_content_subset = tweet_content[:50]\n\nresults = sentiment_analyzer(classifier, tweet_content_subset, candidate_labels)\nresults","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Food for Thought**: \n\n- How is the performance of the model? Comment on the performance of this model compared to the binary model.\n- What do you think about the model performance if you change the subject of the labels from `sentiment` to other subject?","metadata":{}},{"cell_type":"markdown","source":"_Type your answer here, replacing this text._","metadata":{}},{"cell_type":"markdown","source":"### Your Free Time","metadata":{}},{"cell_type":"markdown","source":"**Your tasks**:\n\nYou will add any textual dataset you like, use your own labels by changing the items with **...** and build your own classification model!\n\nFeel free to share your thoughts with your teammates and workshop team.","metadata":{}},{"cell_type":"code","source":"candidate_labels = ...\ntext = ...\n\nresults = sentiment_analyzer(classifier, text, candidate_labels)\nresults","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"![](img/eva-congrats.png)","metadata":{}}]}