{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"anaconda-cloud":{},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":783630,"sourceType":"datasetVersion","datasetId":311962},{"sourceId":7763359,"sourceType":"datasetVersion","datasetId":4540583}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exercise 3: Language models and large language models ","metadata":{}},{"cell_type":"markdown","source":"Attribution: Kolhatkar, Varada (2024) [DSCI 575](https://ubc-mds.github.io/DSCI_575_adv-mach-learn/README.html)","metadata":{}},{"cell_type":"markdown","source":"## Imports <a name=\"im\"></a>","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport sys\nfrom collections import Counter, defaultdict\nfrom urllib.request import urlopen\nfrom hashlib import sha1\n\nimport numpy as np\nimport numpy.random as npr\nimport pandas as pd\n\nfrom transformers import pipeline, AutoTokenizer\n\npd.set_option('display.max_colwidth', 200)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Getting Started with Kaggle Kernels\n<hr>","metadata":{}},{"cell_type":"markdown","source":"We are going to run this notebook on the cloud using [Kaggle](https://www.kaggle.com). To get started, follow these steps:\n\n1. Go to https://www.kaggle.com/kernels\n\n2. Make an account if you don't have one, and verify your phone number (to get access to GPUs)\n3. Select `+ New Notebook`\n4. Go to `File -> Import Notebook`\n5. Upload this notebook\n6. On the right-hand side of your Kaggle notebook, make sure:\n  \n  - `Internet` is enabled.\n\nOnce you've done all your work on Kaggle, you can download the notebook from Kaggle. That way any work you did on Kaggle won't be lost. ","metadata":{}},{"cell_type":"markdown","source":"## Exercise 1: Text Generation using Markov Models\n\n### Character-based Markov model of language\n<hr>\n\nIn this exercise, you will write a class `MarkovModel` to `fit` an n-gram model of language and generated text.\n\nThe starter code below uses the hyperparameter `n`, which represents the state of the Markov model as the last `n` characters of a given string. In Exercise 1, we explored `n=1` (bigram model), where each state of the Markov model was a single character, and the generation of each character was dependent only on the previous character. We aim to incorporate more context to produce more intelligible text. For instance, with `n=3`, the probability distribution for the next character will be based on the preceding three characters. \n\n> Note that `n` in the term n-gram does not exactly correspond to the variable `n` in our implementation below. Instead `n` refers to the number of previous time steps to use as a context. For 2-gram (bigram) the value of the variable `n` is 1 which means considering one previous time step as context. For 4-gram the value of `n` is 3 which means considering three previous time steps as context.    \n\nTo train our model, we record every occurrence of each n-gram and the subsequent character. Then, similar to the approach in naive Bayes, we normalize these counts to probabilities for each n-gram. The `fit` function implements these steps. \n\nTo generate a new sequence, we start with some initial seed at least of length `n`. You can explicitly pass this seed when you call the `generate` method. By default, we will just use the first `n` characters in the training text as the seed, which are saved at the end of the `fit` function. Then, for the current n-gram we will look up the probability distribution over next characters and sample a character according to this distribution.\n\nAttribution: assignment adapted with permission from Princeton COS 126, [_Markov Model of Natural Language_]( http://www.cs.princeton.edu/courses/archive/fall15/cos126/assignments/markov.html). Original assignment was developed by Bob Sedgewick and Kevin Wayne. If you are interested in more background info, you can take a look at the original version. The original paper by Shannon, [A Mathematical Theory of Communication](http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf), essentially created the field of information theory and is one of the best scientific papers ever written (in terms of both impact and readability).  ","metadata":{}},{"cell_type":"markdown","source":"In order to use the recipe [dataset](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions)\n\n1. Click `+ Add Input` at the top right of the notebook. Click on the \"Dataset\"\n\n2. Search for 'food-com-recipes-and-user-interactions'. Several datasets will appear. Look for and 'Add' the dataset with the size of 280MB.\n\n3. Run the follow cells for preparation of the data and model training setup.","metadata":{}},{"cell_type":"code","source":"class MarkovModel:\n    def __init__(self, n):\n        \"\"\"\n        Initialize the Markov model object.\n\n        Parameters:\n        ----------\n        n : int\n            the size of the ngram\n        \"\"\"\n        self.n = n\n        self.probabilities_ = None\n        self.frequencies_ = None\n        self.starting_chars = None\n\n    def fit(self, text):\n        \"\"\"\n        Fit a Markov model and create a transition matrix.\n\n        Parameters\n        ----------\n        text : str\n            a corpus of text\n        \"\"\"\n\n        # Store the first n characters of the training text, as we will use these\n        # to seed our `generate` function\n        self.starting_chars = text[: self.n]\n\n        # Make text circular so markov chain doesn't get stuck\n        circ_text = text + text[: self.n]\n\n        # Step 1: Compute frequencies\n        # count the number of occurrences of each letter following a given n-gram\n        frequencies = defaultdict(Counter)\n        for i in range(len(text)):\n            frequencies[circ_text[i : i + self.n]][circ_text[i + self.n]] += 1.0\n\n        # Step 2: Normalize the frequencies into probabilities\n        self.probabilities_ = defaultdict(dict)\n        for ngram, counts in frequencies.items():\n            self.probabilities_[ngram][\"symbols\"] = list(counts.keys())\n            probs = np.array(list(counts.values()))\n            probs /= np.sum(probs)\n            self.probabilities_[ngram][\"probs\"] = probs\n\n        self.frequencies_ = frequencies  # you never know when this might come in handy\n    \n    def generate(self, seq_len, seed=\"\"):\n        \"\"\"\n        Using self.starting_chars, generate a sequence of length seq_len\n        using the transition matrix created in the fit method.\n\n        Parameters\n        ----------\n        seq_len : int\n            the desired length of the sequence\n        seed : str\n            the seed for text generation\n        Returns:\n        ----------\n        str\n            the generated sequence\n        \"\"\"\n        if not seed:\n            s = self.starting_chars\n        else:\n            s = seed\n\n        while len(s) < seq_len:\n            current_ngram = s[-self.n :]\n            probs = self.probabilities_[current_ngram]\n            s += np.random.choice(probs[\"symbols\"], p=probs[\"probs\"])\n        return s","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up data\nrecipes_file = \"/kaggle/input/food-com-recipes-and-user-interactions/RAW_recipes.csv\"\norig_recipes_df = pd.read_csv(recipes_file)\norig_recipes_df = orig_recipes_df.dropna()\ncorpus = \"\\n\".join(orig_recipes_df[\"name\"].tolist())\nprint(f\"Corpus length: {len(corpus)}\")\nprint(f\"Corpus sample: {corpus[:100]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Novel recipe name generation\n\n**Your tasks:**\n\nIn this exercise, you will train Markov models using the recipe titles in the `corpus` variable above for different values of `n` within the range 1 to 10. For each value of `n`, show generated text comprising at least `100` characters by running the follow cells. \n\nFeel free to select any `n` and `seed` of your choice. Please note that the length of your `seed` should be at least `n`.","metadata":{}},{"cell_type":"code","source":"# Settings\nn = 3\nnum_characters = 100\nseed = 'egg'\n\nchar_model = MarkovModel(n=n)\nchar_model.fit(corpus)\nprint(f\"Text generated with n = {n}:\")\nprint(f\"Generate recipe titles: \\n{char_model.generate(num_characters, seed=seed)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 1.1 \n\n<div class=\"alert alert-info\">\n\n**Discussion questions**\n\n1. How does the value of `n` affect the quality of generated recipe titles?\n2. Experiment with different seeds.\n3. What changes would be required to adapt the model from character-based generation to word-based generation?\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"_Type your answer here, replacing this text._","metadata":{}},{"cell_type":"markdown","source":"## Exercise 2: Sentiment Analysis with LLMs\n<hr>\n\nIn this exercise, you'll apply a large language model (LLM) to real-life use cases. Specifically, you'll build a sentiment analysis model to detect sentiment using a Kaggle dataset [on emotion analysis based on text](https://www.kaggle.com/datasets/saurabhshahane/twitter-sentiment-dataset) without building any machine learning on your own.\n\nIn order to use the dataset.\n\n1. Click `+ Add Input` at the top right of the notebook. Click on the \"Dataset\"\n\n2. Search for 'emotion-analysis-based-on-text'. Several datasets will appear. Look for and 'Add' the dataset with the size of 33MB.\n\n3. Run the follow cell for preparation of the data and model training setup.","metadata":{}},{"cell_type":"code","source":"# Set up data\ntext_data_file = \"/kaggle/input/emotion-analysis-based-on-text/emotion_sentimen_dataset.csv\"\ntext_df = pd.read_csv(text_data_file, index_col=0)\ntext = text_df['text'].tolist()\ntext_df.drop(columns=['Emotion'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Binary Sentiment Analyzer","metadata":{}},{"cell_type":"markdown","source":"Let's predict whether a given text is positive or negative for a sample using a pre-trained large language model (LLM).","metadata":{}},{"cell_type":"code","source":"# Set up model\nmodel_name = \"facebook/bart-large-mnli\"\nclassifier = pipeline(\"zero-shot-classification\", model=model_name)\ncandidate_labels = [\"positive\", \"negative\"]\n\ndef sentiment_analyzer(classifier, text, candidate_labels):\n    results = classifier(text, candidate_labels)\n    return pd.DataFrame(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = sentiment_analyzer(classifier, text[100:120], candidate_labels)\nresults","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Observe the output above. The labels are sorted based on their corresponding scores. A higher score in the scores column indicates that the model predicts a higher probability of the text belonging to the corresponding sentiment label.","metadata":{}},{"cell_type":"markdown","source":"### Exercise 2.1 \n\n<div class=\"alert alert-info\">\n\n**Discussion questions**\n\n1. To what extent do you agree with the model's predictions?\n2. Do you find the corresponding scores helpful?   \n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"_Type your answer here, replacing this text._","metadata":{}},{"cell_type":"markdown","source":"### Multiclass Sentiment Analyzer","metadata":{}},{"cell_type":"markdown","source":"It’s impressive that we were able to perform sentiment analysis without having to build a machine learning model from scratch.\n\nBut what if our goal isn’t sentiment analysis, but emotion classification instead? For example, we might want to assign emotion labels like “joy,” “anger,” “fear,” or “surprise” to a given text.\n\nIn this exercise, you’ll use a pre-trained LLM to classify the emotions expressed in text into different categories based on their content. To start, we’ll focus on four emotions: sadness, joy, anger, and fear. Feel free to experiment by adding more emotions to the list below.","metadata":{}},{"cell_type":"code","source":"candidate_labels = [\"sadness\", \"joy\", \"anger\", \"fear\"]\n\nresults = sentiment_analyzer(classifier, text[100:120], candidate_labels)\nresults","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exercise 2.2\n\n<div class=\"alert alert-info\">\n\n**Discussion questions**\n\n1. To what extent do you agree with the model's predictions?\n2. Do you find the corresponding scores helpful?   \n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"_Type your answer here, replacing this text._","metadata":{}},{"cell_type":"markdown","source":"### Your Free Time (Optional)","metadata":{}},{"cell_type":"markdown","source":"**Your tasks**:\n\nYou can use any text dataset you like and customize the labels by replacing the items marked with ... to build your own classification model!\n\nFeel free to share your ideas and insights with your teammates and the teaching team.","metadata":{}},{"cell_type":"code","source":"candidate_labels = ...\ntext = ...\n\nresults = sentiment_analyzer(classifier, text, candidate_labels)\nresults","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}